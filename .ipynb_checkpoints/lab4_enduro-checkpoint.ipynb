{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yuyang Liang and Antoine Sevec AI Enduro project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "from gym.utils.play import play\n",
    "import pygame\n",
    "import matplotlib\n",
    "import argparse\n",
    "from gym import logger\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  1.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  1.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  7  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  8  reward =  1.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  8  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  4  reward =  0.0 done =  False\n",
      "action =  4  reward =  0.0 done =  False\n",
      "action =  4  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  4  reward =  0.0 done =  False\n",
      "action =  4  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  1  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  1.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  3  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  1.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  2  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n",
      "action =  0  reward =  0.0 done =  False\n"
     ]
    }
   ],
   "source": [
    "def mycallback(obs_t, obs_tp1, action, rew, done, info):\n",
    "    #print(\"action = \", action, \" reward = \", rew, \"done = \", done)\n",
    "    list2 = [action]\n",
    "    vector1 = np.array(list2)\n",
    "    #imageio.imwrite('outfile.png', (obs_t-obs_tp1))#[55:155:2, 20:160:2, 1])\n",
    "    with open('X_enduro.txt', 'a') as outfileX:\n",
    "       np.savetxt(outfileX, delimiter=',', X=(obs_t)[55:155:2, 20:160:2, 1], fmt='%d')\n",
    "    with open('Y_enduro.txt', 'a') as outfileY:\n",
    "        np.savetxt(outfileY, delimiter='', X=vector1, fmt='%d')\n",
    "    #return [action,]\n",
    "\n",
    "#plotter = gym.utils.play.PlayPlot(mycallback, 30 * 5, [\"action\"]) #plot in real-time\n",
    "\n",
    "env = gym.make(\"Enduro-v0\")\n",
    "\n",
    "play(env, zoom=4, fps=30, callback=mycallback)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load the dataset\n",
    "#   X,Y is the training and label dataset\n",
    "size_of_input = [70,50]\n",
    "size_of_label = 9\n",
    "\n",
    "X = np.loadtxt('X_enduro.txt',delimiter=',')\n",
    "Y_data = np.loadtxt('Y_enduro.txt',delimiter=',')\n",
    "X = X.reshape((Y_data.size,size_of_input[0],size_of_input[1],1))\n",
    "\n",
    "Y=np.empty([0,size_of_label])   # size of Y is (number of sample data,9), each column represents the action\n",
    "for i in Y_data:  #label encoding\n",
    "    for j in range(size_of_label):\n",
    "        if i == j:\n",
    "            arr = np.zeros(size_of_label)\n",
    "            arr[j]=1\n",
    "            Y=np.append(Y,arr)\n",
    "            \n",
    "Y=Y.reshape(Y_data.size,size_of_label)\n",
    "\n",
    "# split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=np.random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(size_of_input[0],size_of_input[1],1)))\n",
    "\n",
    "model.add(Conv2D(filters=3, kernel_size=(6, 6), activation='relu',strides=(2, 2)))\n",
    "#model.add(Conv2D(filters=3, kernel_size=(6, 6), activation='relu',strides=(1, 1)))\n",
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2), strides=(3,3), padding='same'))\n",
    "#model.add(Conv2D(filters=1, kernel_size=(2, 2), activation='softmax',strides=(2, 2), padding='same'))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(units=1000, activation='relu'))\n",
    "model.add(Dense(units=200, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=10, activation='sigmoid'))\n",
    "model.add(Dense(units=size_of_label, activation='sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1*10e-4)  #change learning rate here\n",
    "model.summary()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.3087 - categorical_accuracy: 0.9364 - val_loss: 1.0010 - val_categorical_accuracy: 0.6757\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3059 - categorical_accuracy: 0.9391 - val_loss: 0.9861 - val_categorical_accuracy: 0.6811\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3272 - categorical_accuracy: 0.9283 - val_loss: 1.0229 - val_categorical_accuracy: 0.6811\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.3019 - categorical_accuracy: 0.9445 - val_loss: 0.9903 - val_categorical_accuracy: 0.6919\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2970 - categorical_accuracy: 0.9432 - val_loss: 0.9811 - val_categorical_accuracy: 0.6703\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2745 - categorical_accuracy: 0.9459 - val_loss: 1.0178 - val_categorical_accuracy: 0.6865\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2639 - categorical_accuracy: 0.9486 - val_loss: 0.9407 - val_categorical_accuracy: 0.7189\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2581 - categorical_accuracy: 0.9513 - val_loss: 0.9818 - val_categorical_accuracy: 0.7027\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2513 - categorical_accuracy: 0.9513 - val_loss: 1.0127 - val_categorical_accuracy: 0.6919\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2387 - categorical_accuracy: 0.9581 - val_loss: 1.0784 - val_categorical_accuracy: 0.6703\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2272 - categorical_accuracy: 0.9567 - val_loss: 1.1072 - val_categorical_accuracy: 0.6811\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2205 - categorical_accuracy: 0.9581 - val_loss: 1.0500 - val_categorical_accuracy: 0.6811\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2136 - categorical_accuracy: 0.9621 - val_loss: 1.0856 - val_categorical_accuracy: 0.6811\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2232 - categorical_accuracy: 0.9594 - val_loss: 1.0985 - val_categorical_accuracy: 0.6811\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2468 - categorical_accuracy: 0.9472 - val_loss: 1.0945 - val_categorical_accuracy: 0.6541\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.2556 - categorical_accuracy: 0.9432 - val_loss: 1.0687 - val_categorical_accuracy: 0.7081\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2458 - categorical_accuracy: 0.9553 - val_loss: 1.0946 - val_categorical_accuracy: 0.6919\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2578 - categorical_accuracy: 0.9445 - val_loss: 1.0427 - val_categorical_accuracy: 0.6811\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2290 - categorical_accuracy: 0.9594 - val_loss: 1.0910 - val_categorical_accuracy: 0.6649\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2087 - categorical_accuracy: 0.9621 - val_loss: 1.0465 - val_categorical_accuracy: 0.7135\n"
     ]
    }
   ],
   "source": [
    "#%%train the model\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test))\n",
    "#zz=model.predict(x_test) #see the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional : plotting the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3b9bed91afd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \"\"\"\n\u001b[0;32m    352\u001b[0m     \u001b[0m_warn_if_gui_out_of_main_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_backend_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(cls, block)\u001b[0m\n\u001b[0;32m   3542\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3543\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3544\u001b[1;33m             \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3546\u001b[0m     \u001b[1;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\_backend_tk.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[0mmanagers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGcf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmanagers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m             \u001b[0mmanagers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\tkinter\\__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% plot the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  model evaluation\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "pred_test = model.predict(x_test)\n",
    "print(tf.math.confusion_matrix(y_test.argmax(axis=1), pred_test.argmax(axis=1)))\n",
    "\n",
    "# you can save the model here\n",
    "#model.save('pong_game.h5')\n",
    "#%%\n",
    "# use model.predict() to get the prediction from input\n",
    "\n",
    "#model.save('enduro_game1.h5')\n",
    "#model=tf.keras.models.load_model('enduro_game1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run the agent\n",
    "import gym\n",
    "import pygame\n",
    "env = gym.make('Enduro-v0')\n",
    "fps=30 #30 frame per second is default ratio\n",
    "max_iteration = 1000  #number of frames\n",
    "size_of_input = [70,50]\n",
    "zoom = 4\n",
    "#size_of_label = 9\n",
    "\n",
    "def action_mapping(array):\n",
    "    #i = np.nonzero(array)[0][0] #return the index of the action\n",
    "    i = np.argmax(array)\n",
    "    return i\n",
    "\n",
    "obs = env.reset()[55:155:2, 20:160:2, 1].reshape(1,size_of_input[0],size_of_input[1],1)\n",
    "action = np.array([1]) # first action = 0\n",
    "#size_of_input[0],size_of_input[1]\n",
    "\n",
    "for t in range(max_iteration): #the part of prediction is slow due to the tensor operation\n",
    "#while(1):\n",
    "    env.render()\n",
    "    if action[t] !=0:\n",
    "        print(\"action:\",action[t])    \n",
    "    obs,rew,d,inf=env.step(action[t]) # take a predicted action\n",
    "    obs = obs[55:155:2, 20:160:2, 1].reshape(1,size_of_input[0],size_of_input[1],1) #reshape to fit in the input layer of model\n",
    "    action = np.append(action,action_mapping(model.predict(obs)))\n",
    "    if rew != 0:\n",
    "        print(\"reward: \", rew)\n",
    "    #pygame.time.Clock().tick(fps)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
